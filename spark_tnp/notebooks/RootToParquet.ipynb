{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert root files to parquet\n",
    "\n",
    "This notebook demonstrates how to convert root files to the parquet dataformat.\n",
    "For this notebook you will need to connect to the `k8s` spark cluster and enable the options:\n",
    "- `spark.dynamicAllocation.maxExecutors`: `100`\n",
    "- `spark.driver.memory`: `6g`\n",
    "- `spark.jars`: `laurelin-1.0.0.jar,log4j-api-2.13.0.jar,log4j-core-2.13.0.jar`\n",
    "- `spark.kubernetes.container.image`: `gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin`\n",
    "- `spark.kubernetes.container.image.pullPolicy`: `Always`\n",
    "- `spark.driver.extraClassPath`: `./laurelin-1.0.0.jar,./log4j-api-2.13.0.jar,./log4j-core-2.13.0.jar`\n",
    "- Select the \"Include MemoryIntensive options\"\n",
    "\n",
    "After running this notebook, the output will be written to the `/hdfs` user directory the user has permission to use.\n",
    "For example, I have written to my user directory on the `analytix` cluster (`/hdfs/analytix.cern.ch/dntaylor`).\n",
    "After running this conversion, you can use the `analytix` cluster to run your spark jobs on the parquet datafiles,\n",
    "which is faster than running over the `root` dataformat using the `k8s` cluster.\n",
    "\n",
    "Before initializing the spark instance, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-26 18:04:55--  https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.0.0/laurelin-1.0.0.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.112.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.112.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1708408 (1.6M) [application/java-archive]\n",
      "Server file no newer than local file ‘laurelin-1.0.0.jar’ -- not retrieving.\n",
      "\n",
      "--2020-02-26 18:04:55--  https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.13.0/log4j-api-2.13.0.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.112.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.112.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 286261 (280K) [application/java-archive]\n",
      "Server file no newer than local file ‘log4j-api-2.13.0.jar’ -- not retrieving.\n",
      "\n",
      "--2020-02-26 18:04:55--  https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.13.0/log4j-core-2.13.0.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 151.101.112.209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|151.101.112.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1699530 (1.6M) [application/java-archive]\n",
      "Server file no newer than local file ‘log4j-core-2.13.0.jar’ -- not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/1.0.0/laurelin-1.0.0.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.13.0/log4j-api-2.13.0.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.13.0/log4j-core-2.13.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir_Run2017_UL = '/eos/cms/store/group/phys_muon/TagAndProbe/ULRereco/2017/102X'\n",
    "fnamesMap = {\n",
    "    'Z': {\n",
    "        'Run2017_UL': {\n",
    "            'Run2017B': [f for f in glob.glob(os.path.join(baseDir_Run2017_UL, 'Run2017B/tnpZ*.root')) if 'hadd' not in f],\n",
    "            'Run2017C': [f for f in glob.glob(os.path.join(baseDir_Run2017_UL, 'Run2017C/tnpZ*.root')) if 'hadd' not in f],\n",
    "            'Run2017D': [f for f in glob.glob(os.path.join(baseDir_Run2017_UL, 'Run2017D/tnpZ*.root')) if 'hadd' not in f],\n",
    "            'Run2017E': [f for f in glob.glob(os.path.join(baseDir_Run2017_UL, 'Run2017E_99Percent/tnpZ*.root')) if 'hadd' not in f],\n",
    "            'Run2017F': [f for f in glob.glob(os.path.join(baseDir_Run2017_UL, 'Run2017F_99Percent/tnpZ*.root')) if 'hadd' not in f],\n",
    "            'DY_madgraph': [f for f in glob.glob(os.path.join(baseDir_Run2017_UL, 'DY_M50_pdfwgt/tnpZ*.root')) if 'hadd' not in f],\n",
    "        },\n",
    "    },\n",
    "    'JPsi': {\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(resonance,era,subEra):\n",
    "\n",
    "\n",
    "    fnames = ['root://eosuser'+f for f in fnamesMap.get(resonance,{}).get(era,{}).get(subEra,[])]\n",
    "\n",
    "    outDir = os.path.join('parquet',resonance,era,subEra)\n",
    "    outname = os.path.join(outDir,'tnp.parquet')\n",
    "\n",
    "    treename = 'tpTree/fitter_tree'\n",
    "    \n",
    "    # process 1000 files at a time\n",
    "    # this is about the limit that can be handled when writing\n",
    "    batchsize = 1000\n",
    "    new = True\n",
    "    while fnames:\n",
    "        current = fnames[:batchsize]\n",
    "        fnames = fnames[batchsize:]\n",
    "        \n",
    "        rootfiles = spark.read.format(\"root\").option('tree', treename).load(current)\n",
    "        # merge rootfiles. chosen to make files of 8-32 MB (input) become at most 1 GB (parquet recommendation)\n",
    "        # https://parquet.apache.org/documentation/latest/\n",
    "        # .coalesce(int(len(current)/32)) \\\n",
    "        # but it is too slow for now, maybe try again later\n",
    "        if new:\n",
    "            rootfiles.write.parquet(outname)\n",
    "            new = False\n",
    "        else:\n",
    "            rootfiles.write.mode('append').parquet(outname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resonance = 'Z'\n",
    "era = 'Run2017_UL'\n",
    "subEra = 'Run2017F'\n",
    "convert(resonance, era, subEra)\n",
    "subEras = fnamesMap.get(resonance,{}).get(era,{}).keys()\n",
    "#for subEra in subEras:\n",
    "#    print('converting',resonance,era,subEra)\n",
    "#    convert(resonance, era, subEra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive"
   ],
   "list_of_options": [
    {
     "name": "spark.dynamicAllocation.maxExecutors",
     "value": "100"
    },
    {
     "name": "spark.driver.memory",
     "value": "6g"
    },
    {
     "name": "spark.jars",
     "value": "laurelin-1.0.0.jar,log4j-api-2.13.0.jar,log4j-core-2.13.0.jar"
    },
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-1.0.0.jar,./log4j-api-2.13.0.jar,./log4j-core-2.13.0.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
